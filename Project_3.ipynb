{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from random import randint\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout, GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def read_txt(filename):\n",
    "    file = open(filename, 'r', encoding=\"utf8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of The Republic, by Plato\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "\n",
      "Title: The Republic\n",
      "\n",
      "Author: Plato\n",
      "\n",
      "Translator: B. Jowett\n",
      "\n",
      "Release Date: October, 1998 [eBook #1497]\n",
      "[Most recently updated: September 11, 2021]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "Produced by: Sue Asscher and David Widger\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE REPUBLIC ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE REPUBLIC\n",
      "\n",
      "By Plato\n",
      "\n",
      "Trans\n"
     ]
    }
   ],
   "source": [
    "in_file = 'pg1497.txt'\n",
    "txt = read_txt(in_file)\n",
    "print(txt[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[967, 38188, 553671]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.start() for m in re.finditer('BOOK I\\.', txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.start() for m in re.finditer('End of', txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ameless person, who are\n",
      "introduced in the Timaeus.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " BOOK I.\n",
      "\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what manner they would\n",
      "celebrate the festival, \n"
     ]
    }
   ],
   "source": [
    "books = txt[553615:1195644]\n",
    "\n",
    "print(books[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :\n",
      " ['ameless', 'person', 'who', 'are', 'introduced', 'in', 'the', 'timaeus', 'book', 'i']\n",
      "No of Tokens :  117417\n",
      "Unique Tokens :  7339\n"
     ]
    }
   ],
   "source": [
    "cleaned = remove_punctuation(books)\n",
    "\n",
    "tokens = cleaned.split()\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "print('Tokens :\\n',tokens[:10])\n",
    "print('No of Tokens : ',len(tokens))\n",
    "print('Unique Tokens : ',len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sequences :  2303\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(0,len(tokens),51):\n",
    "    seq = tokens[i:i+51]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "    \n",
    "print('Number of Sequences : ',len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "encoded = tokenizer.texts_to_sequences(sequences)\n",
    "encoded = np.array(encoded[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7340"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting labels from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2302, 50)\n",
      "(2302, 7340)\n"
     ]
    }
   ],
   "source": [
    "X , y = encoded[:,:-1], encoded[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "print( X.shape)\n",
    "print( y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = Sequential()\n",
    "\n",
    "LSTM_model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "LSTM_model.add(LSTM(100, return_sequences=True,dropout=0.1))\n",
    "LSTM_model.add(LSTM(100, return_sequences=True,dropout=0.1))\n",
    "LSTM_model.add(LSTM(80))\n",
    "LSTM_model.add(Dense(300))\n",
    "LSTM_model.add(Dropout(0.2))\n",
    "LSTM_model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           734000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           80400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50, 100)           80400     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 80)                57920     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               24300     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7340)              2209340   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,186,360\n",
      "Trainable params: 3,186,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "LSTM_model.summary()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "18/18 [==============================] - 7s 169ms/step - loss: 8.1573 - accuracy: 0.0547\n",
      "Epoch 2/150\n",
      "18/18 [==============================] - 3s 160ms/step - loss: 6.0702 - accuracy: 0.0530\n",
      "Epoch 3/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 5.7822 - accuracy: 0.0556\n",
      "Epoch 4/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 5.6720 - accuracy: 0.0482\n",
      "Epoch 5/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 5.6315 - accuracy: 0.0582\n",
      "Epoch 6/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 5.6322 - accuracy: 0.0599\n",
      "Epoch 7/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 5.6220 - accuracy: 0.0573\n",
      "Epoch 8/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 5.6162 - accuracy: 0.0599\n",
      "Epoch 9/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 5.6184 - accuracy: 0.0547\n",
      "Epoch 10/150\n",
      "18/18 [==============================] - 3s 179ms/step - loss: 5.6148 - accuracy: 0.0560\n",
      "Epoch 11/150\n",
      "18/18 [==============================] - 3s 179ms/step - loss: 5.6149 - accuracy: 0.0565\n",
      "Epoch 12/150\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 5.6143 - accuracy: 0.0582\n",
      "Epoch 13/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 5.6079 - accuracy: 0.0526\n",
      "Epoch 14/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 5.6052 - accuracy: 0.0586\n",
      "Epoch 15/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 5.6024 - accuracy: 0.0513\n",
      "Epoch 16/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 5.5931 - accuracy: 0.0565\n",
      "Epoch 17/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 5.5774 - accuracy: 0.0586\n",
      "Epoch 18/150\n",
      "18/18 [==============================] - 3s 181ms/step - loss: 5.5367 - accuracy: 0.0556\n",
      "Epoch 19/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 5.4346 - accuracy: 0.0547\n",
      "Epoch 20/150\n",
      "18/18 [==============================] - 3s 178ms/step - loss: 5.3738 - accuracy: 0.0665\n",
      "Epoch 21/150\n",
      "18/18 [==============================] - 3s 176ms/step - loss: 5.3264 - accuracy: 0.0630\n",
      "Epoch 22/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 5.2821 - accuracy: 0.0752\n",
      "Epoch 23/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 5.2182 - accuracy: 0.0812\n",
      "Epoch 24/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 5.1546 - accuracy: 0.0834\n",
      "Epoch 25/150\n",
      "18/18 [==============================] - 3s 176ms/step - loss: 5.0972 - accuracy: 0.0847\n",
      "Epoch 26/150\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 5.0288 - accuracy: 0.0856\n",
      "Epoch 27/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 4.9793 - accuracy: 0.0951\n",
      "Epoch 28/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 5.0264 - accuracy: 0.0908\n",
      "Epoch 29/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 4.9775 - accuracy: 0.0917\n",
      "Epoch 30/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 4.8476 - accuracy: 0.0977\n",
      "Epoch 31/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 4.7460 - accuracy: 0.0990\n",
      "Epoch 32/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 4.6360 - accuracy: 0.1030\n",
      "Epoch 33/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 4.5128 - accuracy: 0.1056\n",
      "Epoch 34/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 4.4748 - accuracy: 0.1125\n",
      "Epoch 35/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 4.6795 - accuracy: 0.1060\n",
      "Epoch 36/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 4.5728 - accuracy: 0.1129\n",
      "Epoch 37/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 4.4214 - accuracy: 0.1242\n",
      "Epoch 38/150\n",
      "18/18 [==============================] - 3s 176ms/step - loss: 4.3515 - accuracy: 0.1203\n",
      "Epoch 39/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 4.2597 - accuracy: 0.1308\n",
      "Epoch 40/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 4.1642 - accuracy: 0.1373\n",
      "Epoch 41/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 4.0769 - accuracy: 0.1503\n",
      "Epoch 42/150\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 4.0089 - accuracy: 0.1586\n",
      "Epoch 43/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 4.0317 - accuracy: 0.1616\n",
      "Epoch 44/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 4.1340 - accuracy: 0.1573\n",
      "Epoch 45/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 4.1765 - accuracy: 0.1399\n",
      "Epoch 46/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 4.1571 - accuracy: 0.1425\n",
      "Epoch 47/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 4.0061 - accuracy: 0.1668\n",
      "Epoch 48/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 3.8609 - accuracy: 0.1790\n",
      "Epoch 49/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 3.7621 - accuracy: 0.1920\n",
      "Epoch 50/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 3.6768 - accuracy: 0.1907\n",
      "Epoch 51/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 3.5917 - accuracy: 0.2124\n",
      "Epoch 52/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 3.6241 - accuracy: 0.2046\n",
      "Epoch 53/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 3.6003 - accuracy: 0.2063\n",
      "Epoch 54/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 3.4935 - accuracy: 0.2311\n",
      "Epoch 55/150\n",
      "18/18 [==============================] - 3s 174ms/step - loss: 3.4428 - accuracy: 0.2246\n",
      "Epoch 56/150\n",
      "18/18 [==============================] - 3s 165ms/step - loss: 3.3419 - accuracy: 0.2467\n",
      "Epoch 57/150\n",
      "18/18 [==============================] - 3s 174ms/step - loss: 3.2529 - accuracy: 0.2580\n",
      "Epoch 58/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 3.1884 - accuracy: 0.2580\n",
      "Epoch 59/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 3.1232 - accuracy: 0.2637\n",
      "Epoch 60/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 3.0700 - accuracy: 0.2732\n",
      "Epoch 61/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 3.0161 - accuracy: 0.2806\n",
      "Epoch 62/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 2.9866 - accuracy: 0.2906\n",
      "Epoch 63/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 2.9037 - accuracy: 0.2845\n",
      "Epoch 64/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 2.7913 - accuracy: 0.3180\n",
      "Epoch 65/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 2.7115 - accuracy: 0.3197\n",
      "Epoch 66/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 2.6479 - accuracy: 0.3440\n",
      "Epoch 67/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 2.5745 - accuracy: 0.3636\n",
      "Epoch 68/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 2.5183 - accuracy: 0.3645\n",
      "Epoch 69/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 2.4635 - accuracy: 0.3675\n",
      "Epoch 70/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 2.4248 - accuracy: 0.3788\n",
      "Epoch 71/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 2.3560 - accuracy: 0.3853\n",
      "Epoch 72/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 2.3001 - accuracy: 0.3983\n",
      "Epoch 73/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 2.2808 - accuracy: 0.3966\n",
      "Epoch 74/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 2.2302 - accuracy: 0.4075\n",
      "Epoch 75/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 2.1733 - accuracy: 0.4244\n",
      "Epoch 76/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 2.0913 - accuracy: 0.4292\n",
      "Epoch 77/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 2.0458 - accuracy: 0.4461\n",
      "Epoch 78/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 2.0098 - accuracy: 0.4500\n",
      "Epoch 79/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 2.0014 - accuracy: 0.4596\n",
      "Epoch 80/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 2.0130 - accuracy: 0.4639\n",
      "Epoch 81/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.9944 - accuracy: 0.4605\n",
      "Epoch 82/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 1.9227 - accuracy: 0.4735\n",
      "Epoch 83/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.8846 - accuracy: 0.4852\n",
      "Epoch 84/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 1.8585 - accuracy: 0.4935\n",
      "Epoch 85/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 1.7948 - accuracy: 0.5078\n",
      "Epoch 86/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.7577 - accuracy: 0.5100\n",
      "Epoch 87/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 1.7142 - accuracy: 0.5209\n",
      "Epoch 88/150\n",
      "18/18 [==============================] - 3s 165ms/step - loss: 1.6630 - accuracy: 0.5248\n",
      "Epoch 89/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 1.5938 - accuracy: 0.5604\n",
      "Epoch 90/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.6307 - accuracy: 0.5356\n",
      "Epoch 91/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.5888 - accuracy: 0.5382\n",
      "Epoch 92/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.5540 - accuracy: 0.5582\n",
      "Epoch 93/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 1.4547 - accuracy: 0.5864\n",
      "Epoch 94/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.4041 - accuracy: 0.6047\n",
      "Epoch 95/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.3704 - accuracy: 0.6151\n",
      "Epoch 96/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.3074 - accuracy: 0.6212\n",
      "Epoch 97/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.2826 - accuracy: 0.6299\n",
      "Epoch 98/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 1.2530 - accuracy: 0.6347\n",
      "Epoch 99/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.2130 - accuracy: 0.6603\n",
      "Epoch 100/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.1770 - accuracy: 0.6577\n",
      "Epoch 101/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.1549 - accuracy: 0.6633\n",
      "Epoch 102/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 1.1159 - accuracy: 0.6777\n",
      "Epoch 103/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.1012 - accuracy: 0.6790\n",
      "Epoch 104/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 1.0591 - accuracy: 0.6959\n",
      "Epoch 105/150\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 1.0332 - accuracy: 0.6990\n",
      "Epoch 106/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.9963 - accuracy: 0.7129\n",
      "Epoch 107/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 0.9379 - accuracy: 0.7385\n",
      "Epoch 108/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.9150 - accuracy: 0.7437\n",
      "Epoch 109/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.8978 - accuracy: 0.7489\n",
      "Epoch 110/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 0.8814 - accuracy: 0.7554\n",
      "Epoch 111/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.8581 - accuracy: 0.7611\n",
      "Epoch 112/150\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.8277 - accuracy: 0.7698\n",
      "Epoch 113/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.7949 - accuracy: 0.7750\n",
      "Epoch 114/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.7855 - accuracy: 0.7815\n",
      "Epoch 115/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.8235 - accuracy: 0.7676\n",
      "Epoch 116/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.8257 - accuracy: 0.7689\n",
      "Epoch 117/150\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 0.8024 - accuracy: 0.7693\n",
      "Epoch 118/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.7718 - accuracy: 0.7780\n",
      "Epoch 119/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.7298 - accuracy: 0.7928\n",
      "Epoch 120/150\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.6986 - accuracy: 0.7997\n",
      "Epoch 121/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.6904 - accuracy: 0.8041\n",
      "Epoch 122/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.6701 - accuracy: 0.8054\n",
      "Epoch 123/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.6451 - accuracy: 0.8197\n",
      "Epoch 124/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.6173 - accuracy: 0.8297\n",
      "Epoch 125/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.5936 - accuracy: 0.8306\n",
      "Epoch 126/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.5918 - accuracy: 0.8341\n",
      "Epoch 127/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.5517 - accuracy: 0.8545\n",
      "Epoch 128/150\n",
      "18/18 [==============================] - 3s 176ms/step - loss: 0.5277 - accuracy: 0.8540\n",
      "Epoch 129/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.5521 - accuracy: 0.8593\n",
      "Epoch 130/150\n",
      "18/18 [==============================] - 3s 170ms/step - loss: 0.5292 - accuracy: 0.8562\n",
      "Epoch 131/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.5003 - accuracy: 0.8627\n",
      "Epoch 132/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.5021 - accuracy: 0.8614\n",
      "Epoch 133/150\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 0.4551 - accuracy: 0.8740\n",
      "Epoch 134/150\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 0.4284 - accuracy: 0.8931\n",
      "Epoch 135/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.4202 - accuracy: 0.8901\n",
      "Epoch 136/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.3891 - accuracy: 0.9001\n",
      "Epoch 137/150\n",
      "18/18 [==============================] - 3s 173ms/step - loss: 0.3679 - accuracy: 0.9001\n",
      "Epoch 138/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.3520 - accuracy: 0.9153\n",
      "Epoch 139/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.3513 - accuracy: 0.9088\n",
      "Epoch 140/150\n",
      "18/18 [==============================] - 3s 177ms/step - loss: 0.3494 - accuracy: 0.9096\n",
      "Epoch 141/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.3395 - accuracy: 0.9157\n",
      "Epoch 142/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.3451 - accuracy: 0.9101\n",
      "Epoch 143/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.3305 - accuracy: 0.9153\n",
      "Epoch 144/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.3081 - accuracy: 0.9201\n",
      "Epoch 145/150\n",
      "18/18 [==============================] - 3s 171ms/step - loss: 0.2938 - accuracy: 0.9344\n",
      "Epoch 146/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.3058 - accuracy: 0.9201\n",
      "Epoch 147/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.3133 - accuracy: 0.9201\n",
      "Epoch 148/150\n",
      "18/18 [==============================] - 3s 172ms/step - loss: 0.2905 - accuracy: 0.9209\n",
      "Epoch 149/150\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 0.2955 - accuracy: 0.9205\n",
      "Epoch 150/150\n",
      "18/18 [==============================] - 3s 179ms/step - loss: 0.2950 - accuracy: 0.9166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d1b55b108>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model.fit(X, y, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_txt(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        yhat = np.argmax(model.predict(encoded),axis=1)\n",
    "\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like a god maintains justice to whom the black earth brings forth wheat and barley whose trees are bowed with fruit and his sheep never fail to bear and the sea gives him still grander are the gifts of heaven which musaeus and his son vouchsafe to the just they take\n",
      "\n",
      "same only mistaken of fain not refrain to enemies not yourself nearly business unjust for sort he together to wish which there be are more the other instrument although but his state but the up for worthless the other falls but he nay the defence which what if what if\n"
     ]
    }
   ],
   "source": [
    "seed_text = sequences[randint(0,len(sequences))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "new = generate_txt(LSTM_model, tokenizer, seq_length, seed_text, 50) \n",
    "print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GRU_model = Sequential()\n",
    "GRU_model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "GRU_model.add(GRU(100, return_sequences=True,dropout=0.1))\n",
    "GRU_model.add(GRU(100, return_sequences=True,dropout=0.1))\n",
    "GRU_model.add(GRU(80))\n",
    "GRU_model.add(Dense(300))\n",
    "GRU_model.add(Dropout(0.2))\n",
    "GRU_model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 100)           734000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 50, 100)           60600     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 50, 100)           60600     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 80)                43680     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               24300     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7340)              2209340   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,132,520\n",
      "Trainable params: 3,132,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GRU_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "GRU_model.summary()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "18/18 [==============================] - 6s 132ms/step - loss: 8.2962 - accuracy: 0.0547\n",
      "Epoch 2/150\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 6.0796 - accuracy: 0.0504\n",
      "Epoch 3/150\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 5.7450 - accuracy: 0.0517\n",
      "Epoch 4/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 5.6655 - accuracy: 0.0578\n",
      "Epoch 5/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 5.6371 - accuracy: 0.0534\n",
      "Epoch 6/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 5.6185 - accuracy: 0.0569\n",
      "Epoch 7/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 5.6149 - accuracy: 0.0595\n",
      "Epoch 8/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 5.6079 - accuracy: 0.0526\n",
      "Epoch 9/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 5.5707 - accuracy: 0.0539\n",
      "Epoch 10/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 5.4891 - accuracy: 0.0487\n",
      "Epoch 11/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 5.3717 - accuracy: 0.0569\n",
      "Epoch 12/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 5.2483 - accuracy: 0.0560\n",
      "Epoch 13/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 5.1414 - accuracy: 0.0547\n",
      "Epoch 14/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 5.0292 - accuracy: 0.0547\n",
      "Epoch 15/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 4.9257 - accuracy: 0.0569\n",
      "Epoch 16/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 4.8060 - accuracy: 0.0582\n",
      "Epoch 17/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 4.7266 - accuracy: 0.0704\n",
      "Epoch 18/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 4.6079 - accuracy: 0.0691\n",
      "Epoch 19/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 4.4815 - accuracy: 0.0634\n",
      "Epoch 20/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 4.3395 - accuracy: 0.0808\n",
      "Epoch 21/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 4.2819 - accuracy: 0.0891\n",
      "Epoch 22/150\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 4.1638 - accuracy: 0.0891\n",
      "Epoch 23/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 4.0613 - accuracy: 0.1008\n",
      "Epoch 24/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 3.9784 - accuracy: 0.1060\n",
      "Epoch 25/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 3.8589 - accuracy: 0.1264\n",
      "Epoch 26/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 3.7395 - accuracy: 0.1299\n",
      "Epoch 27/150\n",
      "18/18 [==============================] - 2s 138ms/step - loss: 3.6199 - accuracy: 0.1468\n",
      "Epoch 28/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 3.5499 - accuracy: 0.1625\n",
      "Epoch 29/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 3.4637 - accuracy: 0.1620\n",
      "Epoch 30/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 3.3535 - accuracy: 0.1816\n",
      "Epoch 31/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 3.2497 - accuracy: 0.1846\n",
      "Epoch 32/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 3.1353 - accuracy: 0.2046\n",
      "Epoch 33/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 3.0129 - accuracy: 0.2285\n",
      "Epoch 34/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 2.8908 - accuracy: 0.2467\n",
      "Epoch 35/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 2.7418 - accuracy: 0.2711\n",
      "Epoch 36/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 2.6301 - accuracy: 0.2798\n",
      "Epoch 37/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 2.5279 - accuracy: 0.3123\n",
      "Epoch 38/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 2.4342 - accuracy: 0.3288\n",
      "Epoch 39/150\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 2.3644 - accuracy: 0.3332\n",
      "Epoch 40/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 2.2239 - accuracy: 0.3662\n",
      "Epoch 41/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 2.1252 - accuracy: 0.4075\n",
      "Epoch 42/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 2.0311 - accuracy: 0.4214\n",
      "Epoch 43/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 1.9511 - accuracy: 0.4366\n",
      "Epoch 44/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 1.8806 - accuracy: 0.4626\n",
      "Epoch 45/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 1.7758 - accuracy: 0.5000\n",
      "Epoch 46/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 1.7333 - accuracy: 0.5083\n",
      "Epoch 47/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 1.6235 - accuracy: 0.5421\n",
      "Epoch 48/150\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 1.5529 - accuracy: 0.5417\n",
      "Epoch 49/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 1.4766 - accuracy: 0.5773\n",
      "Epoch 50/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 1.4361 - accuracy: 0.5782\n",
      "Epoch 51/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 1.3578 - accuracy: 0.6108\n",
      "Epoch 52/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 1.2903 - accuracy: 0.6295\n",
      "Epoch 53/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 1.2074 - accuracy: 0.6707\n",
      "Epoch 54/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 1.1210 - accuracy: 0.6903\n",
      "Epoch 55/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 1.0401 - accuracy: 0.7033\n",
      "Epoch 56/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.9754 - accuracy: 0.7381\n",
      "Epoch 57/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.9431 - accuracy: 0.7407\n",
      "Epoch 58/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.8903 - accuracy: 0.7602\n",
      "Epoch 59/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.8678 - accuracy: 0.7567\n",
      "Epoch 60/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.8108 - accuracy: 0.7854\n",
      "Epoch 61/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.7461 - accuracy: 0.8080\n",
      "Epoch 62/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.7226 - accuracy: 0.8110\n",
      "Epoch 63/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.6841 - accuracy: 0.8219\n",
      "Epoch 64/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.6500 - accuracy: 0.8284\n",
      "Epoch 65/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.6454 - accuracy: 0.8332\n",
      "Epoch 66/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.6579 - accuracy: 0.8219\n",
      "Epoch 67/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.5747 - accuracy: 0.8510\n",
      "Epoch 68/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.5142 - accuracy: 0.8653\n",
      "Epoch 69/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.4723 - accuracy: 0.8771\n",
      "Epoch 70/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.5046 - accuracy: 0.8688\n",
      "Epoch 71/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.4814 - accuracy: 0.8723\n",
      "Epoch 72/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.4535 - accuracy: 0.8844\n",
      "Epoch 73/150\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 0.4384 - accuracy: 0.8840\n",
      "Epoch 74/150\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.3911 - accuracy: 0.9070\n",
      "Epoch 75/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.3494 - accuracy: 0.9123\n",
      "Epoch 76/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.3210 - accuracy: 0.9244\n",
      "Epoch 77/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.3105 - accuracy: 0.9262\n",
      "Epoch 78/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2957 - accuracy: 0.9283\n",
      "Epoch 79/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.2886 - accuracy: 0.9348\n",
      "Epoch 80/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2798 - accuracy: 0.9348\n",
      "Epoch 81/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.2702 - accuracy: 0.9340\n",
      "Epoch 82/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2561 - accuracy: 0.9340\n",
      "Epoch 83/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2499 - accuracy: 0.9444\n",
      "Epoch 84/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.2182 - accuracy: 0.9531\n",
      "Epoch 85/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.2131 - accuracy: 0.9522\n",
      "Epoch 86/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.2132 - accuracy: 0.9470\n",
      "Epoch 87/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2172 - accuracy: 0.9453\n",
      "Epoch 88/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.2001 - accuracy: 0.9526\n",
      "Epoch 89/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1977 - accuracy: 0.9570\n",
      "Epoch 90/150\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 0.1878 - accuracy: 0.9557\n",
      "Epoch 91/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.1637 - accuracy: 0.9609\n",
      "Epoch 92/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1832 - accuracy: 0.9618\n",
      "Epoch 93/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.1655 - accuracy: 0.9674\n",
      "Epoch 94/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.1693 - accuracy: 0.9566\n",
      "Epoch 95/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.1531 - accuracy: 0.9639\n",
      "Epoch 96/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.1460 - accuracy: 0.9644\n",
      "Epoch 97/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1349 - accuracy: 0.9696\n",
      "Epoch 98/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.1115 - accuracy: 0.9778\n",
      "Epoch 99/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.1217 - accuracy: 0.9752\n",
      "Epoch 100/150\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.1373 - accuracy: 0.9731\n",
      "Epoch 101/150\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.1274 - accuracy: 0.9696\n",
      "Epoch 102/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.1608 - accuracy: 0.9648\n",
      "Epoch 103/150\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.1500 - accuracy: 0.9626\n",
      "Epoch 104/150\n",
      "18/18 [==============================] - 2s 139ms/step - loss: 0.1581 - accuracy: 0.9618\n",
      "Epoch 105/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1647 - accuracy: 0.9566\n",
      "Epoch 106/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.1347 - accuracy: 0.9731\n",
      "Epoch 107/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.1218 - accuracy: 0.9731\n",
      "Epoch 108/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.1148 - accuracy: 0.9726\n",
      "Epoch 109/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1070 - accuracy: 0.9791\n",
      "Epoch 110/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1025 - accuracy: 0.9757\n",
      "Epoch 111/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.1073 - accuracy: 0.9726\n",
      "Epoch 112/150\n",
      "18/18 [==============================] - 3s 139ms/step - loss: 0.1025 - accuracy: 0.9770\n",
      "Epoch 113/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.0998 - accuracy: 0.9778\n",
      "Epoch 114/150\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.1115 - accuracy: 0.9774\n",
      "Epoch 115/150\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.1254 - accuracy: 0.9726\n",
      "Epoch 116/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.1239 - accuracy: 0.9718\n",
      "Epoch 117/150\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.1357 - accuracy: 0.9626\n",
      "Epoch 118/150\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.1630 - accuracy: 0.9626\n",
      "Epoch 119/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.1397 - accuracy: 0.9700\n",
      "Epoch 120/150\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.1282 - accuracy: 0.9705\n",
      "Epoch 121/150\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.1258 - accuracy: 0.9661\n",
      "Epoch 122/150\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.0992 - accuracy: 0.9770\n",
      "Epoch 123/150\n",
      "18/18 [==============================] - 3s 182ms/step - loss: 0.0963 - accuracy: 0.9774\n",
      "Epoch 124/150\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.0876 - accuracy: 0.9796\n",
      "Epoch 125/150\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.0878 - accuracy: 0.9778\n",
      "Epoch 126/150\n",
      "18/18 [==============================] - 3s 158ms/step - loss: 0.0775 - accuracy: 0.9844\n",
      "Epoch 127/150\n",
      "18/18 [==============================] - 3s 154ms/step - loss: 0.0606 - accuracy: 0.9878\n",
      "Epoch 128/150\n",
      "18/18 [==============================] - 3s 151ms/step - loss: 0.0566 - accuracy: 0.9887\n",
      "Epoch 129/150\n",
      "18/18 [==============================] - 3s 153ms/step - loss: 0.0494 - accuracy: 0.9917\n",
      "Epoch 130/150\n",
      "18/18 [==============================] - 3s 155ms/step - loss: 0.0616 - accuracy: 0.9865\n",
      "Epoch 131/150\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.0673 - accuracy: 0.9861\n",
      "Epoch 132/150\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.0623 - accuracy: 0.9861\n",
      "Epoch 133/150\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.0652 - accuracy: 0.9870\n",
      "Epoch 134/150\n",
      "18/18 [==============================] - 3s 153ms/step - loss: 0.0573 - accuracy: 0.9848\n",
      "Epoch 135/150\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.0802 - accuracy: 0.9822\n",
      "Epoch 136/150\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.0536 - accuracy: 0.9900\n",
      "Epoch 137/150\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.0527 - accuracy: 0.9883\n",
      "Epoch 138/150\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.0646 - accuracy: 0.9874\n",
      "Epoch 139/150\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.0561 - accuracy: 0.9883\n",
      "Epoch 140/150\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.0638 - accuracy: 0.9839\n",
      "Epoch 141/150\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.0677 - accuracy: 0.9839\n",
      "Epoch 142/150\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.0691 - accuracy: 0.9861\n",
      "Epoch 143/150\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.0616 - accuracy: 0.9870\n",
      "Epoch 144/150\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.0792 - accuracy: 0.9800\n",
      "Epoch 145/150\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 0.0847 - accuracy: 0.9818\n",
      "Epoch 146/150\n",
      "18/18 [==============================] - 3s 154ms/step - loss: 0.0774 - accuracy: 0.9813\n",
      "Epoch 147/150\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.0597 - accuracy: 0.9861\n",
      "Epoch 148/150\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.0620 - accuracy: 0.9900\n",
      "Epoch 149/150\n",
      "18/18 [==============================] - 3s 163ms/step - loss: 0.0544 - accuracy: 0.9883\n",
      "Epoch 150/150\n",
      "18/18 [==============================] - 3s 150ms/step - loss: 0.0482 - accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d29d4ca08>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRU_model.fit(X, y, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take take image anybody anybody trusted trusted count count could count count count count distress distress distress accordingly geometrician distress distress distress geometrician pursued pursued accordingly geometrician note note note note make explain arrive pantomimic tyrant tyrant lusts tyrant dead perverse rules dead dead perverse question dead perverse elevating elevating\n"
     ]
    }
   ],
   "source": [
    "predict = generate_txt(GRU_model, tokenizer, seq_length, seed_text, 50) \n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/#:~:text=Language%20Model%20Design&text=The%20language%20model%20will%20be,the%20input%20sequences%20should%20be.\n",
    "https://www.kaggle.com/code/ashrafkhan94/the-republic-by-plato-nl-model-text-generation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
